{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-image:url(images/bertinoro.jpg); padding: 10px 30px 20px 30px; background-size:cover; background-opacity:50%; border-radius:5px\">\n",
    "<p style=\"float:right; margin-top:20px; padding: 20px 20px 0px 20px; background:rgba(255,255,255,0.6); border-radius:10px;\">\n",
    "<img width=\"400px\" src=images/obspy_logo_full_524x179px.png?raw=true>\n",
    "</p>\n",
    "\n",
    "<h1 style=\"color:#333\">First TIDES Training School</h1>\n",
    "<h5 style=\"color:#FFF\">Bertinoro (FC), Italy, June 1-5 2015</h5>\n",
    "\n",
    "<h3 style=\"color:#EEE\">Day 1: Data Acquisition and Processing with ObsPy</h3>\n",
    "\n",
    "<h2 style=\"color:#FFF\">Preparing data for Noise Spectra and Polarization Practical</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the practical on Tuesday we will find the direction of dominant oceanic noise sources, and see how they evolve along the year. For this, we will use continuous 3-component records from one of the stations in the Northern Italian network.  \n",
    "We will download the data, remove the instrument response and cut them into equal length pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from __future__ import print_function\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import obspy\n",
    "from obspy.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import os\n",
    "import glob\n",
    "\n",
    "noise_folder = \"/home/tides/Desktop/data/noise_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 1: get the data\n",
    "- We can download the data from the INGV web service.  \n",
    "- We will use station \"TEOL\" in Northern Italy. First, let's have a look at this station's response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = Client(\"INGV\")\n",
    "station = 'TEOL'\n",
    "network = 'IV'\n",
    "\n",
    "# get the instrument response\n",
    "inv = client.get_stations(network=network, station=station, channel=\"BH?\", level=\"response\")\n",
    "inv.plot_response(0.001);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we want to see how the noise sources evolve along the year, we will use several days of continuous data per season. The start dates of each **'season'** are set here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = UTCDateTime(year=2014, julday=30)  #winter\n",
    "season = \"winter\"\n",
    "\n",
    "#t = UTCDateTime(year=2014, julday=70)  # spring\n",
    "#season = \"spring\"\n",
    "\n",
    "#t = UTCDateTime(year=2013, julday=180)   # summer\n",
    "#season = \"summer\"\n",
    "\n",
    "#t = UTCDateTime(year=2013, julday=280) # fall\n",
    "#season = \"fall\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Option 1: Download the data from the INGV webservice:\n",
    "You can uncomment the following block of code to download the data. However, if everyone tries to do this at the same time, we might run in to problems. The raw data for 10 days per season is therefore also provided on the virtual box. A bit further down in this notebook, you can also choose to read the provided data from disk (see option 2). \n",
    "\n",
    "- We will download 10 days of data per season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ndays = 10\n",
    "#\n",
    "#print(\"Downloading data....\")\n",
    "#\n",
    "#st = obspy.Stream()\n",
    "#for day in range(ndays):\n",
    "#    try:\n",
    "#        st += client.get_waveforms(network=network, station=station, location=\"\", channel=\"BH?\",\n",
    "#                                   starttime=t + day * 86400, endtime=t + (day + 1) * 86400)\n",
    "#    except:\n",
    "#        print(\"No data for %s\" % (t + day * 86400).strftime(\"%Y-%m-%d\"))\n",
    "#        pass\n",
    "#\n",
    "#st.attach_response(inv)\n",
    "#print(st)\n",
    "#st.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uncomment the folowing to save the raw data you just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw_folder = os.path.join(noise_folder, 'raw', \"%s-%s-%s\" % (t.year, station, season))\n",
    "#if not os.path.exists(raw_folder):\n",
    "#        os.makedirs(raw_folder)\n",
    "#\n",
    "#for tr in st: \n",
    "#    filename = tr.id + tr.stats.starttime.strftime(\"_%Y.%m.%d-%H.%M\") + \".SAC\"\n",
    "#    tr.write(os.path.join(raw_folder, filename), format=\"SAC\") \n",
    "#    \n",
    "#print(\"Done. Raw data written to: %s\", raw_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: read the data from disk:\n",
    "\n",
    "- This will read the data which is already stored on the virtual box, instead of downloading.   \n",
    "If you are using **option 1** above, comment out this block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = obspy.Stream()\n",
    "\n",
    "raw_folder = os.path.join(noise_folder, 'raw', \"%s-%s-%s\" % (t.year, station, season))\n",
    "list = glob.glob(os.path.join(raw_folder, \"*%s*.SAC\" % station))\n",
    "\n",
    "for filename in list:\n",
    "    st += obspy.read(filename)\n",
    "    \n",
    "print(\"Finished reading. Available waveforms: \\n\", st)\n",
    "st.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##Step 2: Clean the data\n",
    "- Now prepare to remove the instrument response.  \n",
    "We will downsample the data to 1Hz sampling rate afterwards, so make sure the pre-filtering you apply **eliminates frequencies below 0.5Hz!**  \n",
    "**Note**: the actual response correction will take place later, on shorter timewindows, in order to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the frequency ranges for the pre-filter:\n",
    "f2 = 1.0 / 300.0 #max period\n",
    "f3 = 1.0 / 3.0 #min period\n",
    "f1 = 0.8 * f2\n",
    "f4 = 1.3 * f3\n",
    "pre_filt = (f1, f2, f3, f4)\n",
    "\n",
    "# Removing the response for the whole 10 day stream is very slow. \n",
    "# If you really want to try, you can uncomment the following:\n",
    "#for tr in st:\n",
    "#    tr.remove_response(pre_filt=pre_filt, output=\"VEL\")\n",
    "#st.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 3: cut down seismograms into smaller pieces and save\n",
    "\n",
    "- The polarization scripts we will use on Tuesday only accept signals with less than **8192 samples**. We will cut down our signals to 2-hour chunks.  \n",
    "- After cutting out the chunks, we downsample the data from 20Hz to 1Hz. This will speed up subsequent processing.  \n",
    "- We also make sure that the signals from all 3 components start and end at the same time (within the diff_threshold of 0.05 seconds)\n",
    "- To save the data, first we have to create a directory structure with daily directories per station. The directory structure will also be built in this loop.  \n",
    "**Example: Directory structure & filename**    \n",
    "data/noise/2014-TEOL-winter/SAC_TRACES/2014.001/2014.001.00.00.TEOL.LHZ.sac   \n",
    "data/noise/2014-TEOL-winter/SAC_TRACES/2014.002/2014.002.00.00.TEOL.LHZ.sac  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# length of chunks in seconds:\n",
    "length = 7200\n",
    "# difference between start and endtime allowed, in seconds:\n",
    "diff_threshold = 0.05\n",
    "\n",
    "stf = st.copy()\n",
    "stf.merge(fill_value=\"interpolate\")\n",
    "\n",
    "# find the earliest and latest times included in our complete signal\n",
    "starttime = max([tr.stats.starttime for tr in stf])\n",
    "endtime = min([tr.stats.endtime for tr in stf])\n",
    "\n",
    "time = UTCDateTime(starttime.year, starttime.month, starttime.day)\n",
    "\n",
    "stf.attach_response(inv)\n",
    "\n",
    "while time < endtime:\n",
    "    if length > 8192:\n",
    "        print(\"WARNING: signal length must be less than 8192 samples. \\\n",
    "                Please set a smaller value for variable 'length'\")\n",
    "        break\n",
    "    \n",
    "    # start cutting the chunks\n",
    "    st2 = stf.slice(starttime=time, endtime=time + length)\n",
    "\n",
    "    time += length\n",
    "    if not st2:\n",
    "        continue\n",
    "        \n",
    "    if len(set(tr.stats.npts for tr in st2)) != 1:\n",
    "        continue\n",
    "        \n",
    "    if abs((st2[0].stats.endtime - st2[0].stats.starttime) - length) > 1:\n",
    "        continue\n",
    "\n",
    "    #check if all three components start and end at the same time. If not, go to the next interval\n",
    "    if(sum(np.diff(sorted(tr.stats.starttime for tr in st2)))) > diff_threshold:\n",
    "        continue\n",
    "               \n",
    "    if(sum(np.diff(sorted(tr.stats.endtime for tr in st2)))) > diff_threshold:\n",
    "        continue\n",
    "\n",
    "    # correct for instrument response\n",
    "    st2.remove_response(pre_filt=pre_filt, output=\"VEL\")\n",
    "    # downsample the data from 20Hz to a sampling frequency of 1Hz\n",
    "    st2.decimate(factor=20, no_filter=True)\n",
    "    \n",
    "    # create directory structure\n",
    "    year_folder = os.path.join(noise_folder, time.strftime(\"%Y-\") + station + \"-\" + season, \"SAC_TRACES\")\n",
    "    day_folder = os.path.join(year_folder, time.strftime(\"%Y.%j\"))\n",
    "        \n",
    "    if not os.path.exists(year_folder):\n",
    "        os.makedirs(year_folder)\n",
    "        \n",
    "    if not os.path.exists(day_folder):\n",
    "        os.makedirs(day_folder)\n",
    "        \n",
    "    # save in sac format:\n",
    "    for tr in st2: \n",
    "        filename = \"%s.%s.%s.SAC\" % (tr.stats.starttime.strftime(\"%Y.%j-%H.%M\"), station, tr.stats.channel)\n",
    "        sacfile = os.path.join(day_folder, filename)\n",
    "        tr.write(sacfile, format='SAC')\n",
    "        \n",
    "print(\"Done. Files written to: %s\" % year_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's have a look at the spectrum for one day: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = stf[0].stats.starttime + 86400\n",
    "st2 = stf.slice(starttime=time, endtime= time + length)\n",
    "st2.remove_response(pre_filt=pre_filt, output=\"VEL\")\n",
    "\n",
    "data = np.fft.rfft(st2[0].data)\n",
    "plt.loglog(np.fft.rfftfreq(st2[0].stats.npts) * int(st2[0].stats.sampling_rate), np.abs(data))\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('amplitude')\n",
    "plt.xlim([1.0/400,0.45])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 4: Repeat for other seasons\n",
    "- Now we have prepared the data for the winter of 2014. \n",
    "- **Exercise:**  \n",
    "To prepare the rest of the data, please go back up to the top, and uncomment the next date and season in the second window in \"Step 1\". After that, run the rest of the notebook again to download and prepare the data for each season. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
